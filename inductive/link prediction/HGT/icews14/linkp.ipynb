{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(\n",
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import HGTConv\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected, RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = 'icews14'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd9e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGTEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels_dict, hidden_channels, out_channels, metadata, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = HGTConv(in_channels_dict, hidden_channels, metadata, heads=num_heads)\n",
    "        self.conv2 = HGTConv({k: hidden_channels for k in in_channels_dict}, out_channels, metadata, heads=num_heads)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a03b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        return torch.sigmoid(self.mlp(x_i * x_j)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e257bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, predictor, data, edge_type):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    z_dict = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    pos_edge_index = data[edge_type].edge_label_index\n",
    "    # neg_edge_index = data[edge_type].neg_edge_label_index\n",
    "    \n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data[edge_type].edge_index,\n",
    "        num_nodes=data[edge_type[0]].num_nodes,\n",
    "        num_neg_samples=data[edge_type].edge_label_index.size(1),\n",
    "        method='sparse'\n",
    "    )\n",
    "\n",
    "    src_pos, dst_pos = pos_edge_index\n",
    "    src_neg, dst_neg = neg_edge_index\n",
    "\n",
    "    pos_pred = predictor(z_dict[edge_type[0]][src_pos], z_dict[edge_type[2]][dst_pos])\n",
    "    neg_pred = predictor(z_dict[edge_type[0]][src_neg], z_dict[edge_type[2]][dst_neg])\n",
    "\n",
    "    pred = torch.cat([pos_pred, neg_pred]).cpu()\n",
    "    label = torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)]).cpu()\n",
    "\n",
    "    auc = roc_auc_score(label, pred)\n",
    "    ap = average_precision_score(label, pred)\n",
    "    return auc, ap\n",
    "\n",
    "\n",
    "def train(model, predictor, data, optimizer, edge_type):\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z_dict = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    pos_edge_index = data[edge_type].edge_label_index\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=pos_edge_index,\n",
    "        num_nodes=data[edge_type[0]].num_nodes,\n",
    "        num_neg_samples=pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    src_pos, dst_pos = pos_edge_index\n",
    "    src_neg, dst_neg = neg_edge_index\n",
    "\n",
    "    pos_pred = predictor(z_dict[edge_type[0]][src_pos], z_dict[edge_type[2]][dst_pos])\n",
    "    neg_pred = predictor(z_dict[edge_type[0]][src_neg], z_dict[edge_type[2]][dst_neg])\n",
    "\n",
    "    pred = torch.cat([pos_pred, neg_pred])\n",
    "    label = torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)])\n",
    "\n",
    "    loss = F.binary_cross_entropy(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6484636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_example():\n",
    "#     # Build a toy hetero graph\n",
    "#     data = HeteroData()\n",
    "#     data['user'].x = torch.randn(100, 32)\n",
    "#     data['item'].x = torch.randn(200, 32)\n",
    "\n",
    "#     edge_index = torch.randint(0, 100, (2, 500))  # 500 user-item links\n",
    "#     data['user', 'rates', 'item'].edge_index = edge_index\n",
    "\n",
    "#     # Make it undirected and split\n",
    "#     transform = ToUndirected()  # optional, depends on the task\n",
    "#     data = transform(data)\n",
    "#     split = RandomLinkSplit(\n",
    "#         edge_types=[('user', 'rates', 'item')],\n",
    "#         rev_edge_types=[('item', 'rev_rates', 'user')],\n",
    "#         add_negative_train_samples=True\n",
    "#     )\n",
    "#     train_data, val_data, test_data = split(data)\n",
    "\n",
    "#     metadata = train_data.metadata()\n",
    "#     in_channels_dict = {k: v.size(-1) for k, v in train_data.x_dict.items()}\n",
    "\n",
    "#     # Model\n",
    "#     encoder = HGTEncoder(in_channels_dict, hidden_channels=64, out_channels=64, metadata=metadata).to('cpu')\n",
    "#     predictor = LinkPredictor(in_dim=64).to('cpu')\n",
    "#     optimizer = torch.optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr=0.005)\n",
    "\n",
    "#     # Training loop\n",
    "#     edge_type = ('user', 'rates', 'item')\n",
    "#     for epoch in range(1, 51):\n",
    "#         loss = train(encoder, predictor, train_data, optimizer, edge_type)\n",
    "#         if epoch % 10 == 0:\n",
    "#             val_auc, val_ap = evaluate(encoder, predictor, val_data, edge_type)\n",
    "#             print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
    "\n",
    "#     test_auc, test_ap = evaluate(encoder, predictor, test_data, edge_type)\n",
    "#     print(f\"Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}\")\n",
    "\n",
    "# run_example()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a176dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist = pd.read_csv(f'../../../../data/raw/{dataset}/1-indexed/actor_actor.csv', encoding='utf-8', names=['userID','artistID', 'weight'],)\n",
    "user_friend = pd.read_csv(f'../../../../data/raw/{dataset}/1-indexed/actor_action.csv', encoding='utf-8', names=['userID', 'friendID'])\n",
    "user_tag = pd.read_csv(f'../../../../data/raw/{dataset}/1-indexed/actor_sector.csv', encoding='utf-8', names=['artistID', 'tagID'])\n",
    "\n",
    "# indices = np.arange(len(user_artist))\n",
    "# train_idx, test_idx = train_test_split(indices, test_size=0.15, random_state=42)\n",
    "# val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)\n",
    "# train_data, val_data, test_data = create_data(user_artist, train_idx, val_idx, test_idx)\n",
    "\n",
    "num_actor1 = user_artist['userID'].max()+1\n",
    "num_actor2 = user_artist['artistID'].max()+2\n",
    "num_action = user_friend['friendID'].max()+1\n",
    "num_sector = user_tag['tagID'].max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090f3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def canonicalize_edges(edge_list):\n",
    "    return list(tuple(sorted(e)) for e in edge_list)\n",
    "\n",
    "def split_train_test_edges_hetero(data, remove_fraction):\n",
    "    random.seed(42)\n",
    "\n",
    "    train_data, val_data, test_data = data\n",
    "\n",
    "    for edge_type in train_data.edge_types:\n",
    "        if 'edge_label_index' not in train_data[edge_type]:\n",
    "            continue  # Skip if no label index (not involved in link prediction)\n",
    "\n",
    "        # Extract positive edges only\n",
    "        train_pos_mask = train_data[edge_type].edge_label == 1\n",
    "        train_pos_edges = train_data[edge_type].edge_label_index[:, train_pos_mask]\n",
    "        original_edges = canonicalize_edges(train_pos_edges.t().tolist())\n",
    "\n",
    "        # Get test positives\n",
    "        test_pos_mask = test_data[edge_type].edge_label == 1\n",
    "        test_edges = set(canonicalize_edges(test_data[edge_type].edge_label_index[:, test_pos_mask].t().tolist()))\n",
    "        \n",
    "        # Remove overlap with test\n",
    "        intersection_indices = [i for i, edge in enumerate(original_edges) if edge in test_edges]\n",
    "        num_to_remove = int(len(intersection_indices) * remove_fraction)\n",
    "        remove_indices = set(random.sample(intersection_indices, num_to_remove))\n",
    "        filtered_edges = [edge for i, edge in enumerate(original_edges) if i not in remove_indices]\n",
    "\n",
    "        # Repeat for val positives\n",
    "        val_pos_mask = val_data[edge_type].edge_label == 1\n",
    "        val_edges = set(canonicalize_edges(val_data[edge_type].edge_label_index[:, val_pos_mask].t().tolist()))\n",
    "        intersection_indices = [i for i, edge in enumerate(filtered_edges) if edge in val_edges]\n",
    "        num_to_remove = int(len(intersection_indices) * remove_fraction)\n",
    "        remove_indices = set(random.sample(intersection_indices, num_to_remove))\n",
    "        final_filtered_edges = [edge for i, edge in enumerate(filtered_edges) if i not in remove_indices]\n",
    "\n",
    "        # Update train_data for this edge type\n",
    "        all_edges = train_data[edge_type].edge_label_index\n",
    "        all_labels = train_data[edge_type].edge_label\n",
    "\n",
    "        neg_mask = all_labels == 0\n",
    "        neg_edges = all_edges[:, neg_mask]\n",
    "\n",
    "        # Make sure we have enough negatives\n",
    "        num_pos = len(final_filtered_edges)\n",
    "        if neg_edges.size(1) >= num_pos:\n",
    "            sampled_neg_indices = random.sample(range(neg_edges.size(1)), num_pos)\n",
    "            sampled_neg_edges = neg_edges[:, sampled_neg_indices]\n",
    "        else:\n",
    "            raise ValueError(f\"Not enough negative edges ({neg_edges.size(1)}) to match positives ({num_pos})\")\n",
    "\n",
    "        final_pos_tensor = torch.tensor(final_filtered_edges).t()\n",
    "        train_data[edge_type].edge_label_index = torch.cat([final_pos_tensor, neg_edges], dim=1)\n",
    "        train_data[edge_type].edge_label = torch.cat([\n",
    "            torch.ones(final_pos_tensor.size(1), dtype=torch.long),\n",
    "            torch.zeros(sampled_neg_edges.size(1), dtype=torch.long)\n",
    "        ])\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e181fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "data['actor1'].x = torch.randn(num_actor1, 32)\n",
    "data['actor2'].x = torch.randn(num_actor2, 32)\n",
    "data['action'].x = torch.randn(num_action, 32)\n",
    "data['sector'].x = torch.randn(num_sector, 32)\n",
    "\n",
    "edge_index = torch.tensor(user_artist[[user_artist.columns[0], user_artist.columns[1]]].values.T, dtype=torch.long)\n",
    "\n",
    "data['actor1', 'interacts', 'actor2'].edge_index = torch.tensor(user_artist[[user_artist.columns[0], user_artist.columns[1]]].values.T, dtype=torch.long)\n",
    "data['actor1', 'involved', 'action'].edge_index = torch.tensor(user_friend[[user_friend.columns[0], user_friend.columns[1]]].values.T, dtype=torch.long)\n",
    "data['actor1', 'belongs', 'sector'].edge_index = torch.tensor(user_tag[[user_tag.columns[0], user_tag.columns[1]]].values.T, dtype=torch.long)\n",
    "\n",
    "transform = ToUndirected()  # optional, depends on the task\n",
    "data = transform(data)\n",
    "\n",
    "# split = RandomLinkSplit(\n",
    "#     edge_types=[('actor1', 'interacts', 'actor2'),('actor1', 'involved', 'action'),('actor1', 'belongs', 'sector')],\n",
    "#     rev_edge_types=[('actor2', 'rev_interacts', 'actor1'),('action', 'rev_involved', 'actor1'),('sector', 'rev_belongs', 'actor1')],\n",
    "#     add_negative_train_samples=True\n",
    "# )\n",
    "\n",
    "# train_data, val_data, test_data = split(data)\n",
    "\n",
    "edge_types = [('actor1', 'interacts', 'actor2'),('actor1', 'involved', 'action'),('actor1', 'belongs', 'sector')]\n",
    "rev_edge_types = [('actor2', 'rev_interacts', 'actor1'),('action', 'rev_involved', 'actor1'),('sector', 'rev_belongs', 'actor1')]\n",
    "t,v,te = RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        add_negative_train_samples=True,\n",
    "        edge_types=edge_types,\n",
    "        rev_edge_types=rev_edge_types\n",
    "    )(data)\n",
    "train_data, val_data, test_data = split_train_test_edges_hetero([t,v,te], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae7d960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6877, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(te[('actor1', 'involved', 'action')].edge_label == 0).nonzero().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2520d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  actor1={ x=[7077, 32] },\n",
       "  actor2={ x=[7077, 32] },\n",
       "  action={ x=[200, 32] },\n",
       "  sector={ x=[2227, 32] },\n",
       "  (actor1, interacts, actor2)={\n",
       "    edge_index=[2, 55025],\n",
       "    edge_label=[60266],\n",
       "    edge_label_index=[2, 85158],\n",
       "  },\n",
       "  (actor1, involved, action)={\n",
       "    edge_index=[2, 55025],\n",
       "    edge_label=[42118],\n",
       "    edge_label_index=[2, 76084],\n",
       "  },\n",
       "  (actor1, belongs, sector)={\n",
       "    edge_index=[2, 55025],\n",
       "    edge_label=[18140],\n",
       "    edge_label_index=[2, 64095],\n",
       "  },\n",
       "  (actor2, rev_interacts, actor1)={ edge_index=[2, 55025] },\n",
       "  (action, rev_involved, actor1)={ edge_index=[2, 55025] },\n",
       "  (sector, rev_belongs, actor1)={ edge_index=[2, 55025] }\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05fa89db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ∩ Val: 1\n",
      "Train ∩ Test: 4\n",
      "Val ∩ Test: 1057\n"
     ]
    }
   ],
   "source": [
    "def edge_set(data, etype):\n",
    "    return set(map(tuple, data[etype].edge_label_index.t().tolist()))\n",
    "\n",
    "etype = ('actor1', 'interacts', 'actor2')\n",
    "train_edges = edge_set(train_data, etype)\n",
    "val_edges = edge_set(val_data, etype)\n",
    "test_edges = edge_set(test_data, etype)\n",
    "\n",
    "print(\"Train ∩ Val:\", len(train_edges & val_edges))\n",
    "print(\"Train ∩ Test:\", len(train_edges & test_edges))\n",
    "print(\"Val ∩ Test:\", len(val_edges & test_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968ad006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test():\n",
    "    metadata = train_data.metadata()\n",
    "    in_channels_dict = {k: v.size(-1) for k, v in train_data.x_dict.items()}\n",
    "\n",
    "    # Model\n",
    "    encoder = HGTEncoder(in_channels_dict, hidden_channels=64, out_channels=64, metadata=metadata).to('cpu')\n",
    "    predictor = LinkPredictor(in_dim=64).to('cpu')\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(predictor.parameters()), lr=0.005)\n",
    "\n",
    "    # Training loop\n",
    "    edge_type = ('actor1', 'interacts', 'actor2')\n",
    "    for epoch in range(1, 51):\n",
    "        loss = train(encoder, predictor, train_data, optimizer, edge_type)\n",
    "        if epoch % 10 == 0:\n",
    "            val_auc, val_ap = evaluate(encoder, predictor, val_data, edge_type)\n",
    "            print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
    "\n",
    "    test_auc, test_ap = evaluate(encoder, predictor, test_data, edge_type)\n",
    "    print(f\"Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4292817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010, Loss: 0.6917, Val AUC: 0.5885, AP: 0.5991\n",
      "Epoch 020, Loss: 0.6712, Val AUC: 0.6443, AP: 0.6932\n",
      "Epoch 030, Loss: 0.6562, Val AUC: 0.6664, AP: 0.7196\n",
      "Epoch 040, Loss: 0.6489, Val AUC: 0.6723, AP: 0.7260\n",
      "Epoch 050, Loss: 0.6455, Val AUC: 0.6743, AP: 0.7289\n",
      "Test AUC: 0.6746, Test AP: 0.7294\n"
     ]
    }
   ],
   "source": [
    "train_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
