{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(\n",
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "/home/nxz190009/miniconda3/envs/htgnn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Implements the link prediction task on the FB15k237 datasets according to the\n",
    "`\"Modeling Relational Data with Graph Convolutional Networks\"\n",
    "<https://arxiv.org/abs/1703.06103>`_ paper.\n",
    "\n",
    "Caution: This script is executed in a full-batch fashion, and therefore needs\n",
    "to run on CPU (following the experimental setup in the official paper).\n",
    "\"\"\"\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import RelLinkPredDataset\n",
    "from torch_geometric.nn import GAE, RGCNConv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Training on GPU causes OOM error\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "dataset = 'icews18'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7fff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist = pd.read_csv(f'../../../../data/raw/{dataset}/1-indexed/actor_actor.csv', encoding='utf-8', names=['userID','artistID', 'weight'],)\n",
    "user_friend = pd.read_csv(f'../../../../data/raw/{dataset}/1-indexed/actor_action.csv', encoding='utf-8', names=['userID', 'friendID'])\n",
    "artist_tag = pd.read_csv(f'../../../../data/raw/{dataset}/1-indexed/actor_sector.csv', encoding='utf-8', names=['artistID', 'tagID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ac401",
   "metadata": {},
   "source": [
    "For heterograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253377c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist = user_artist[['userID','artistID']]\n",
    "\n",
    "user_artist['artistID'] += user_artist['userID'].max()\n",
    "user_friend['friendID'] += user_artist['artistID'].max()\n",
    "artist_tag['artistID'] += user_artist['userID'].max()\n",
    "artist_tag['tagID'] += user_friend['friendID'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95a6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_edge_tensors(df, rel_id):\n",
    "    edge_index = torch.tensor(df.values.T, dtype=torch.long)  # shape [2, num_edges]\n",
    "    edge_type = torch.full((edge_index.size(1),), rel_id, dtype=torch.long)\n",
    "    return edge_index, edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a86998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_DICT = {\n",
    "'actor_actor': 0,\n",
    "'actor_action': 1,\n",
    "'actor_sector': 2\n",
    "}\n",
    "\n",
    "edge_index_1, edge_type_1 = df_to_edge_tensors(user_artist, RELATION_DICT['actor_actor'])\n",
    "edge_index_2, edge_type_2 = df_to_edge_tensors(user_friend, RELATION_DICT['actor_action'])\n",
    "edge_index_3, edge_type_3 = df_to_edge_tensors(artist_tag, RELATION_DICT['actor_sector'])\n",
    "\n",
    "# Concatenate all edge indices and types\n",
    "edge_index = torch.cat([edge_index_1, edge_index_2, edge_index_3], dim=1)\n",
    "edge_type = torch.cat([edge_type_1, edge_type_2, edge_type_3], dim=0)\n",
    "\n",
    "# Infer number of nodes\n",
    "num_nodes = edge_index.max().item() + 1\n",
    "\n",
    "data = Data(\n",
    "    edge_index=edge_index,\n",
    "    edge_type=edge_type,\n",
    "    num_nodes=num_nodes\n",
    ")\n",
    "\n",
    "edge_idx = np.arange(edge_index.shape[1])\n",
    "train_idx, test_idx = train_test_split(edge_idx, test_size=0.1, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=42)\n",
    "\n",
    "data.train_edge_index = edge_index[:, train_idx]\n",
    "data.train_edge_type = edge_type[train_idx]\n",
    "\n",
    "data.valid_edge_index = edge_index[:, val_idx]\n",
    "data.valid_edge_type = edge_type[val_idx]\n",
    "\n",
    "data.test_edge_index = edge_index[:, test_idx]\n",
    "data.test_edge_type = edge_type[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389418da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 220530], edge_type=[220530], num_nodes=19140, train_edge_index=[2, 178629], train_edge_type=[178629], valid_edge_index=[2, 19848], valid_edge_type=[19848], test_edge_index=[2, 22053], test_edge_type=[22053])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5865b81",
   "metadata": {},
   "source": [
    "##### Works on CPU so this has been commented out\n",
    "\n",
    "Training on all these 220530 edges results in Out-of-Memory error and so reducing it to 200000 edges as ICEWS14 has around that number of edges and the training on ICEWS14 was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b7366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all edges and types\n",
    "# all_edge_index = torch.cat([\n",
    "#     data.train_edge_index, \n",
    "#     data.valid_edge_index, \n",
    "#     data.test_edge_index\n",
    "# ], dim=1)\n",
    "\n",
    "# all_edge_type = torch.cat([\n",
    "#     data.train_edge_type, \n",
    "#     data.valid_edge_type, \n",
    "#     data.test_edge_type\n",
    "# ])\n",
    "\n",
    "# # Total number of edges\n",
    "# num_total_edges = all_edge_index.size(1)\n",
    "\n",
    "# # Determine how many to keep\n",
    "# num_keep = 200000\n",
    "\n",
    "# # Random permutation of indices\n",
    "# perm = torch.randperm(num_total_edges)[:num_keep]\n",
    "\n",
    "# # Subset the edges\n",
    "# new_edge_index = all_edge_index[:, perm]\n",
    "# new_edge_type = all_edge_type[perm]\n",
    "\n",
    "# # Optional: If needed, re-split into train/val/test using some ratio\n",
    "# num_train = int(0.8 * num_keep)\n",
    "# num_val = int(0.1 * num_keep)\n",
    "# num_test = num_keep - num_train - num_val\n",
    "\n",
    "# train_edge_index = new_edge_index[:, :num_train]\n",
    "# train_edge_type = new_edge_type[:num_train]\n",
    "\n",
    "# valid_edge_index = new_edge_index[:, num_train:num_train + num_val]\n",
    "# valid_edge_type = new_edge_type[num_train:num_train + num_val]\n",
    "\n",
    "# test_edge_index = new_edge_index[:, num_train + num_val:]\n",
    "# test_edge_type = new_edge_type[num_train + num_val:]\n",
    "\n",
    "# # Update data object\n",
    "# data.train_edge_index = train_edge_index\n",
    "# data.train_edge_type = train_edge_type\n",
    "# data.valid_edge_index = valid_edge_index\n",
    "# data.valid_edge_type = valid_edge_type\n",
    "# data.test_edge_index = test_edge_index\n",
    "# data.test_edge_type = test_edge_type\n",
    "\n",
    "# data.edge_index = new_edge_index\n",
    "# data.edge_type = new_edge_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e168d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, hidden_channels, num_relations):\n",
    "        super().__init__()\n",
    "        self.node_emb = Parameter(torch.empty(num_nodes, hidden_channels))\n",
    "        self.conv1 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations,\n",
    "                              num_blocks=5)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.node_emb)\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = self.node_emb\n",
    "        x = self.conv1(x, edge_index, edge_type).relu_()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DistMultDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_relations, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.rel_emb = Parameter(torch.empty(num_relations, hidden_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.rel_emb)\n",
    "\n",
    "    def forward(self, z, edge_index, edge_type):\n",
    "        z_src, z_dst = z[edge_index[0]], z[edge_index[1]]\n",
    "        rel = self.rel_emb[edge_type]\n",
    "        return torch.sum(z_src * rel * z_dst, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d62c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAE(\n",
    "    RGCNEncoder(data.num_nodes, 500, len(RELATION_DICT)*2),\n",
    "    DistMultDecoder(len(RELATION_DICT), 500),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def negative_sampling(edge_index, num_nodes):\n",
    "    # Sample edges by corrupting either the subject or the object of each edge.\n",
    "    mask_1 = torch.rand(edge_index.size(1)) < 0.5\n",
    "    mask_2 = ~mask_1\n",
    "\n",
    "    neg_edge_index = edge_index.clone()\n",
    "    neg_edge_index[0, mask_1] = torch.randint(num_nodes, (mask_1.sum(), ),\n",
    "                                              device=neg_edge_index.device)\n",
    "    neg_edge_index[1, mask_2] = torch.randint(num_nodes, (mask_2.sum(), ),\n",
    "                                              device=neg_edge_index.device)\n",
    "    return neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780c66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_link_predictor(data_edge_, data):\n",
    "\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy()), average_precision_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_rank(ranks):\n",
    "    # fair ranking prediction as the average\n",
    "    # of optimistic and pessimistic ranking\n",
    "    true = ranks[0]\n",
    "    optimistic = (ranks > true).sum() + 1\n",
    "    pessimistic = (ranks >= true).sum()\n",
    "    return (optimistic + pessimistic).float() * 0.5\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_mrr(z, edge_index, edge_type):\n",
    "    ranks = []\n",
    "    for i in tqdm(range(edge_type.numel())):\n",
    "        (src, dst), rel = edge_index[:, i], edge_type[i]\n",
    "\n",
    "        # Try all nodes as tails, but delete true triplets:\n",
    "        tail_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            tail_mask[tails[(heads == src) & (types == rel)]] = False\n",
    "\n",
    "        tail = torch.arange(data.num_nodes)[tail_mask]\n",
    "        tail = torch.cat([torch.tensor([dst]), tail])\n",
    "        head = torch.full_like(tail, fill_value=src)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(tail, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        rank = compute_rank(out)\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # Try all nodes as heads, but delete true triplets:\n",
    "        head_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        for (heads, tails), types in [\n",
    "            (data.train_edge_index, data.train_edge_type),\n",
    "            (data.valid_edge_index, data.valid_edge_type),\n",
    "            (data.test_edge_index, data.test_edge_type),\n",
    "        ]:\n",
    "            head_mask[heads[(tails == dst) & (types == rel)]] = False\n",
    "\n",
    "        head = torch.arange(data.num_nodes)[head_mask]\n",
    "        head = torch.cat([torch.tensor([src]), head])\n",
    "        tail = torch.full_like(head, fill_value=dst)\n",
    "        eval_edge_index = torch.stack([head, tail], dim=0)\n",
    "        eval_edge_type = torch.full_like(head, fill_value=rel)\n",
    "\n",
    "        out = model.decode(z, eval_edge_index, eval_edge_type)\n",
    "        rank = compute_rank(out)\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return (1. / torch.tensor(ranks, dtype=torch.float)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd97fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    pos_out = model.decode(z, data.train_edge_index, data.train_edge_type)\n",
    "\n",
    "    neg_edge_index = negative_sampling(data.train_edge_index, data.num_nodes)\n",
    "    neg_out = model.decode(z, neg_edge_index, data.train_edge_type)\n",
    "\n",
    "    out = torch.cat([pos_out, neg_out])\n",
    "    gt = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\n",
    "    cross_entropy_loss = F.binary_cross_entropy_with_logits(out, gt)\n",
    "    reg_loss = z.pow(2).mean() + model.decoder.rel_emb.pow(2).mean()\n",
    "    loss = cross_entropy_loss + 1e-2 * reg_loss\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    pred = out.sigmoid().detach().cpu().numpy()\n",
    "    labels = gt.detach().cpu().numpy()\n",
    "    auc = roc_auc_score(labels, pred)\n",
    "    ap = average_precision_score(labels, pred)\n",
    "\n",
    "\n",
    "    return float(loss), auc, ap\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    valid_mrr = compute_mrr(z, data.valid_edge_index, data.valid_edge_type)\n",
    "    test_mrr = compute_mrr(z, data.test_edge_index, data.test_edge_type)\n",
    "\n",
    "    return valid_mrr, test_mrr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_auc_ap():\n",
    "    model.eval()\n",
    "    z = model.encode(data.edge_index, data.edge_type)\n",
    "\n",
    "    pos_out = model.decode(z, data.test_edge_index, data.test_edge_type)\n",
    "\n",
    "    neg_edge_index = negative_sampling(data.test_edge_index, data.num_nodes)\n",
    "    neg_out = model.decode(z, neg_edge_index, data.test_edge_type)\n",
    "\n",
    "    out = torch.cat([pos_out, neg_out])\n",
    "    gt = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\n",
    "\n",
    "    pred = out.sigmoid().detach().cpu().numpy()\n",
    "    labels = gt.detach().cpu().numpy()\n",
    "    auc = roc_auc_score(labels, pred)\n",
    "    ap = average_precision_score(labels, pred)\n",
    "\n",
    "    print(f'Test evaluation: AUC : {auc:.6f}, AP : {ap:.6f}')\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4315a0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 220530], edge_type=[220530], num_nodes=19140, train_edge_index=[2, 178629], train_edge_type=[178629], valid_edge_index=[2, 19848], valid_edge_type=[19848], test_edge_index=[2, 22053], test_edge_type=[22053])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "497c25a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001, Loss: 0.6932, AUC : 0.561213, AP : 0.519383\n",
      "Epoch: 00002, Loss: 0.6916, AUC : 0.692809, AP : 0.710774\n",
      "Epoch: 00003, Loss: 0.6632, AUC : 0.791359, AP : 0.797991\n",
      "Epoch: 00004, Loss: 0.4977, AUC : 0.913549, AP : 0.900574\n",
      "Epoch: 00005, Loss: 0.3825, AUC : 0.937929, AP : 0.925314\n",
      "Epoch: 00006, Loss: 2.5089, AUC : 0.654920, AP : 0.700099\n",
      "Epoch: 00007, Loss: 0.4737, AUC : 0.930173, AP : 0.919723\n",
      "Epoch: 00008, Loss: 0.3580, AUC : 0.945113, AP : 0.935692\n",
      "Epoch: 00009, Loss: 0.3850, AUC : 0.945780, AP : 0.936153\n",
      "Epoch: 00010, Loss: 0.3656, AUC : 0.925710, AP : 0.919666\n",
      "Epoch: 00011, Loss: 0.3291, AUC : 0.936024, AP : 0.926897\n",
      "Epoch: 00012, Loss: 0.3377, AUC : 0.935357, AP : 0.927498\n",
      "Epoch: 00013, Loss: 0.2999, AUC : 0.951047, AP : 0.945340\n",
      "Epoch: 00014, Loss: 0.2711, AUC : 0.957961, AP : 0.951129\n",
      "Epoch: 00015, Loss: 0.2626, AUC : 0.957989, AP : 0.947394\n",
      "Epoch: 00016, Loss: 0.2499, AUC : 0.960600, AP : 0.947638\n",
      "Epoch: 00017, Loss: 0.2284, AUC : 0.966148, AP : 0.957032\n",
      "Epoch: 00018, Loss: 0.2379, AUC : 0.968629, AP : 0.962336\n",
      "Epoch: 00019, Loss: 0.2204, AUC : 0.969793, AP : 0.963407\n",
      "Epoch: 00020, Loss: 0.2072, AUC : 0.972030, AP : 0.966258\n",
      "Epoch: 00021, Loss: 0.2143, AUC : 0.969174, AP : 0.961838\n",
      "Epoch: 00022, Loss: 0.2039, AUC : 0.973043, AP : 0.967512\n",
      "Epoch: 00023, Loss: 0.2001, AUC : 0.975437, AP : 0.970239\n",
      "Epoch: 00024, Loss: 0.1961, AUC : 0.976177, AP : 0.971251\n",
      "Epoch: 00025, Loss: 0.1883, AUC : 0.976348, AP : 0.970784\n",
      "Epoch: 00026, Loss: 0.1844, AUC : 0.977971, AP : 0.972336\n",
      "Epoch: 00027, Loss: 0.1855, AUC : 0.977145, AP : 0.971516\n",
      "Epoch: 00028, Loss: 0.1776, AUC : 0.978484, AP : 0.973223\n",
      "Epoch: 00029, Loss: 0.1771, AUC : 0.978961, AP : 0.974123\n",
      "Epoch: 00030, Loss: 0.1720, AUC : 0.980164, AP : 0.975441\n",
      "Epoch: 00031, Loss: 0.1662, AUC : 0.981097, AP : 0.976716\n",
      "Epoch: 00032, Loss: 0.1664, AUC : 0.980790, AP : 0.975941\n",
      "Epoch: 00033, Loss: 0.1617, AUC : 0.981702, AP : 0.977215\n",
      "Epoch: 00034, Loss: 0.1600, AUC : 0.981904, AP : 0.977464\n",
      "Epoch: 00035, Loss: 0.1576, AUC : 0.982616, AP : 0.978534\n",
      "Epoch: 00036, Loss: 0.1564, AUC : 0.982897, AP : 0.979133\n",
      "Epoch: 00037, Loss: 0.1535, AUC : 0.983365, AP : 0.979575\n",
      "Epoch: 00038, Loss: 0.1520, AUC : 0.983805, AP : 0.980049\n",
      "Epoch: 00039, Loss: 0.1513, AUC : 0.983791, AP : 0.979844\n",
      "Epoch: 00040, Loss: 0.1475, AUC : 0.984256, AP : 0.980438\n",
      "Epoch: 00041, Loss: 0.1479, AUC : 0.984449, AP : 0.980571\n",
      "Epoch: 00042, Loss: 0.1490, AUC : 0.985255, AP : 0.981995\n",
      "Epoch: 00043, Loss: 0.1491, AUC : 0.984898, AP : 0.980924\n",
      "Epoch: 00044, Loss: 0.1505, AUC : 0.984572, AP : 0.980401\n",
      "Epoch: 00045, Loss: 0.1485, AUC : 0.984026, AP : 0.979351\n",
      "Epoch: 00046, Loss: 0.1418, AUC : 0.985728, AP : 0.982335\n",
      "Epoch: 00047, Loss: 0.1491, AUC : 0.983525, AP : 0.978979\n",
      "Epoch: 00048, Loss: 0.1403, AUC : 0.986118, AP : 0.983000\n",
      "Epoch: 00049, Loss: 0.1490, AUC : 0.984998, AP : 0.981016\n",
      "Epoch: 00050, Loss: 0.1407, AUC : 0.986187, AP : 0.982915\n",
      "Median time per epoch: 2.8955s\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "data.to(device)\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 51):\n",
    "    start = time.time()\n",
    "    loss, auc, ap = train()\n",
    "    print(f'Epoch: {epoch:05d}, Loss: {loss:.4f}, AUC : {auc:.6f}, AP : {ap:.6f}')\n",
    "    if (epoch % 100) == 0:\n",
    "        valid_mrr, test_mrr = test()\n",
    "        print(f'Val MRR: {valid_mrr:.4f}, Test MRR: {test_mrr:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea6d584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation: AUC : 0.972764, AP : 0.971500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9727639472748495, 0.9714999968229275)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_auc_ap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c892e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
